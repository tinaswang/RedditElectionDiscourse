#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""

This file takes a .json file of submissions generated by Pushshift and downloads
all the comments that get deltas.


For more information on querying Pushshift,see https://github.com/pushshift/api


Grabbed submissions containing "trump" from between
1/1/2016 and 11/7/2016 (the day before the election)
https://api.pushshift.io/reddit/search/submission/?q=trump&after=1200d&before=888d&subreddit=changemyview&size=2000
https://api.pushshift.io/reddit/search/submission/?q=clinton&after=1201d&before=889d&subreddit=changemyview&size=2000&q:not=trump
186 for "clinton" w/out trump
@author: tinaswang

Note: this is how to get all comments by DeltaBot (we ended up not using this)
https://api.pushshift.io/reddit/search/comment/?author=DeltaBot&sort=asc&size=10000000
"""

import time
import datetime
import csv
import sys
import codecs
import pandas as pd
import sys
import os
import re
import numpy as np
import urllib.request
import json
if __name__ == '__main__':

    submission = []
    # substitute in the name of the file we want to look at
    with open('other_submissions.json') as json_file:  
        trump_data = json.load(json_file)["data"]

    LENGTH = len(trump_data)
    print("Length of input: ", LENGTH)

    
    comment_url = "https://api.pushshift.io/reddit/submission/comment_ids/"
    author_url = "https://api.pushshift.io/reddit/comment/search?ids="
    
    # fname is where we will store the resulting file names
    fname = "nonpolitical/other"

    # Occasionally the loop breaks because some submissions 
    # have been deleted since 2016; in those cases there'll be an error like:
    # urllib.error.HTTPError: HTTP Error 400: Bad Request
    # 
    for i in range(0, LENGTH):
        print("i : ", i)
        # get the current id of the submission and check whether
        # there was a delta awarded in the thread
        curr_id = trump_data[i]["id"]
        print("curr_id: ", curr_id)
        curr_url = comment_url + curr_id
        
        try: 
            with urllib.request.urlopen(curr_url) as url:
                data = json.loads(url.read().decode())["data"]
                a_url = author_url + ','.join(data)

                # Make an API request to get all the comments for the submission
                with urllib.request.urlopen(a_url) as url2:
                    all_comment_data = json.loads(url2.read().decode())

                    # check if there is a delta-receiving comment in the submission
                    for x in range(len(all_comment_data["data"])):
                        if (all_comment_data["data"][x]["author"] == "DeltaBot" and 
                            "awarded" in all_comment_data["data"][x]["body"]):
                                print("Added to submission")
                                submission.append(curr_id)
                                print("writing comment data to file")
                                fname_new = fname + "_" + curr_id + ".json"
                                with open(fname_new, 'w') as outfile:
                                    json.dump(all_comment_data, outfile, indent=4)
                                # move on to next submission
                                break
        # It is bad coding practice to catch all possible errors like below but there
        # were multiple similar errors that would sometimes pop up because
        # of deleted submissions and other server issues
        except:
            continue
            

    print("Complete.")
    print("Length of all submissions: ", submission)
    np.savetxt("submission.out", submission)



